\documentclass{article}

\usepackage[margin=0.7in]{geometry}
\usepackage[parfill]{parskip}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsfonts,amsthm} % Math packages

\usepackage{cleveref} % Inline referencing
\usepackage[backend=biber,style=alphabetic]{biblatex} % For bibliography
\addbibresource{bibliography.bib}

\usepackage{setspace} % For double-spacing
\doublespacing

% Theorems/Definitions
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

% Macros
\newcommand\RR{\mathbb R}
\newcommand\RRE{\overline{\mathbb R}}
\newcommand\abs[1]{\left\lvert#1\right\rvert}
\newcommand\fl[1]{\left\lfloor#1\right\rfloor}
\newcommand\Li{\mathrm{Li}}

\title{Bachmann-Landau Notation}
\date{}

\begin{document}

\maketitle

\section{Introduction}\label{sec:introduction}

Paul Bachmann (1837-1920) and Edmund Landau (1877-1930) were German mathematicians who are both remembered for their numerous contributions to number theory.
It was in Bachmann's book \textit{Die Analytische Zahlentheorie}\footnote{Translates literally to \textit{Analytic Number Theory}.} \cite{Bachmann1894} on analytic number theory, a branch of mathematics best characterised by the application of analytic methods to number theoretic problems, that big-$O$ notation was first introduced to mathematics, which he used for handling the error terms in his asymptotic estimates.
Inspired by Bachmann's idea, Landau introduced a stronger variant of big-$O$ notation, which we now call little-$o$, in his two-volume treatise titled \textit{Handbuch der Lehre von der Verteilung der Primzahlen}\footnote{Translates to \textit{Handbook of the theory of distribution of prime numbers}.} \cite{Landau1909}.
Today, these two notations, along with other similarly defined notions (like \(\Theta\)-, \(\omega\)-, or \(\Omega\)-notation), are used in a wide variety of areas ranging, of course, from analytic fields like analytic number theory to even computer science.

I myself have often come across Bachmann-Landau notation in several contexts, but in each such instance, I have had to look up and struggle with their definitions since their meaning used to escape me whenever they appeared in equations.
The usage of big-$O$ and little-$o$ seems to be intuitively clear to mathematicians yet communicating it properly often poses a difficulty.
Thus this investigation is directed towards understanding big-$O$ and little-$o$ notation for what they are and especially how they are to be applied in appropriate mathematical contexts.

\section{Theory}\label{sec:theory}

Both big-$O$ and little-$o$ notation describe the size of a function relative to another function ``in the limit''.
Usually, this limit point is taken to be \(\infty\) (\textit{infinite} asymptotics), or \(0\) (\textit{infinitesimal} asymptotics).
The definition given here generalises this notion to any limit point \(a\in\RRE\), but the main use cases will remain \(a=0\) and \(a=\infty\).
Here \(\RRE\) represents the \textit{extended real numbers}, namely \(\RRE = \RR \cup \{\pm\infty\}\).
A good reference for the basic properties of the extended reals would be the beginning of \cite{Rudin1953}.

\subsection{Big-$O$}\label{ssec:bigo}

There are generally two ways to define each notation in the Bachmann-Landau family of notations: a direct limit-based approach, and a quantifier-based approach.

\begin{definition}
    Given \(a\in\RRE\) and real functions \(f,g\) that are defined on a neighbourhood of \(a\) such that \(g\) is nonzero on this neighbourhood, we say that \(f(x) = O_a(g(x))\) or \(f(x) = O(g(x))\) as \(x\to a\) if one of the following equivalent conditions hold:
    \begin{enumerate}
        \item there exists \(\delta\in(0,\infty)\) such that
            \[\sup_{N_{\delta}(a)} \abs{\frac{f(x)}{g(x)}} < \infty;\]
        \item there exist \(k,\delta\in(0,\infty)\) such that \(x\in N_{\delta}(a)\) implies \(\abs{f(x)} \leq k\abs{g(x)}\).
    \end{enumerate}
\end{definition}

Here \(N_{\delta}(d)\) denotes the neighbourhood of \(a\) of radius \(\delta\), which is \((a-\delta,a+\delta)\) for finite \(a\).
For infinite values, we can define it differently: set \(N_{\delta}(\infty) = (\delta,\infty)\) and \(N_{\delta}(-\infty) = (-\infty,-\delta)\).

The set implicit in the first supremum is
\[S_{\delta} = \left\{\abs{\frac{f(x)}{g(x)}} : x \in (a-\delta,a+\delta)\right\}.\]
Let \(s_{\delta} = \sup S_{\delta}\).
If \(s_{\delta} < \infty\) for some \(\delta\), then as \(s_{\delta}\) is an upper bound of \(S_{\delta}\), we have \(\abs{f(x)} \leq s_{\delta}\abs{g(x)}\) for all \(x\) such that \(x\in N_{\delta}\); thus the first condition implies the second.
Conversely, suppose that there exist \(k,\delta\in(0,\infty)\) such that \(x\in N_{\delta}(a)\) implies \(\abs{f(x)} \leq k\abs{g(x)}\).
Then \(k\) is an upper bound of \(S_{\delta}\), and the least-upper-bound property of \(\RR\) then shows that \(S_{\delta}\) has a finite supremum.
Thus the two conditions are equivalent.\footnote{As is apparent in this argument, the first condition basically says that \(\abs{f(x)/g(x)}\) should be bounded above in some neighbourhood of \(a\).}

Note that \(g\) is required to be nonzero in order for the fraction \(f(x)/g(x)\) to make sense, but that it is sufficient that it be nonzero over some neighbourhood of \(a\) for the definition to work: we can rewrite the definition to ask for such a \(\delta\) that is less than the radius of the neighbourhood in which \(g\) is nonzero.

The definition found in \cite{Landau1909} is for \(a=\infty\) only, since analytic number theory is concerned primarily with infinite limits; the case \(a = 0\), on the other hand, is useful in analysis, for example when truncating the higher order terms in a Taylor series.
As the below examples show, \(O_a\) is rather useless for finite nonzero values of \(a\).
But the above definition is still preferred to defining \(O_0\) and \(O_{\infty}\) separately because it highlights the ``sameness'' of both notions.

When it comes to Bachmann-Landau notations, however, much more important than the rigorous definition is the understanding.
The right way to understand big-$O$ is that big-$O$ is to functions what \(\le\) is to numbers:
\begin{center}
    \(f(x) = O_a(g(x))\) means \(f(x) \le g(x)\) in the limit, up to a constant factor. \\
    (\(f\) does not dominate \(g\) \(\iff\) \(a\) is not greater than \(b\))
\end{center}

\textbf{Examples:}

\begin{itemize}
    \item Let us verify that \(100x^2 = O_{\infty}(x^3)\) using both conditions.
        Using the first condition, for any \(\delta\in(0,\infty)\) we have
        \[\sup_{x>\delta}\abs{\frac{100x^2}{x^3}} = \sup_{x>\delta}\frac{100}{x} = \frac{100}{\delta} < \infty.\]
        Using the second condition, we know that for \(x>1\) we have \(100x^2 \leq 100x^3\).
        In terms of the above analogy, we can observe that \(100x^2\) is ``less than or equal to'' \(x^3\) in the limit to infinity, as \(x^2\) grows slower than \(x^3\) as \(x\to\infty\).
        
    \item If \(f\) and \(g\) are functions that are each either even or odd, then \(f = O_{\infty}(g)\) if and only if \(f = O_{-\infty}(g)\).
        Thus \(100x^2 = O_{-\infty}(x^3)\).

    \item But \(100x^2 \ne O_0(x^3)\); otherwise, there would exist \(\delta,k\in(0,\infty)\) such that \(\abs x < \delta\) implies \(x \geq k/100\), which is absurd.
        We do have, however, that \(x^3 = O_0(100x^2)\), as for \(\abs x < 100\) we have \(x^3 \leq 100x^2\).
        There is a graphical way to understand the relations \(100x^2 = O_{\infty}(x^3)\) and \(x^3 = O_0(100x^2)\).
        The graph of \(100x^2\) is below the graph of \(x^3\) after \(x=100\), meaning that \(100x^2 \leq x^3\) as \(x\to\infty\); similarly, as the graph of \(x^3\) is below the graph of \(100x^2\) for all \(x<100\), we see that \(x^3 \leq 100x^2\) as \(x\to 0\).

    \item To give a more exaggerated example, we have \(x^2 = O_{\infty}(x^{100})\) but \(x^{100} = O_0(x^2)\).

    \item Given a finite \(a\), if \(f\) does not have any asymptote at \(a\) and \(g\) is nonzero in some neighbourhood of \(a\), then we have \(f = O_a(g)\).
        This is because \(\abs{f/g}\) achieves the supremum over a sufficiently small neighbourhood.
\end{itemize}

It is thus evident that \(O_a\) is not very interesting except when \(a=0\) or \(a=\infty\), so it makes sense that the symbol \(O_a\) is not standard; the literature generally exhibits \(f(x) = O(g(x))\) as \(x\to a\), or even hides the ``as \(x\to a\)'' since the value of \(a\) is usually clear from the context (either \(0\) or \(\infty\)).
We will omit the \(a\) subscripts from now on, and sometimes, as in the following theorem, the variable \(x\) too.

\begin{theorem}[Transitivity]
    If \(f = O(g)\) and \(g = O(h)\), then \(f = O(h)\).\footnote{If \(a\) is not specified, neither explicitly nor from context, as in this theorem, then assume any arbitrary \(a\in\RRE\).}
\end{theorem}

\begin{proof}
    By definition, there exist \(k,l,\delta,\epsilon\in(0,\infty)\) such that \(\abs{f(x)} \leq k\abs{g(x)}\) for all \(x\in N_{\delta}(a)\) and \(\abs{g(x)} \leq l\abs{h(x)}\) for all \(x\in N_{\epsilon}(a)\).
    It follows that
    \[\abs{f(x)} \leq k\abs{g(x)} \leq kl\abs{h(x)}\]
    for all \(x\in N_{\min(\delta,\epsilon)}(a)\).
\end{proof}

\subsection{Little-$o$}\label{ssec:littleo}

Once we have an analogue of \(\leq\) for functions, we are motivated to consider if such a notion exists for \(<\).
This gives rise to little-\(o\).

\begin{definition}
    Given \(a\in\RRE\) and real functions \(f,g\) defined on a neighbourhood of \(a\) such that \(g\) is nonzero on this neighbourhood, we say that \(f(x) = o_a(g(x))\) or \(f(x) = o(g(x))\) as \(x\to a\) if one of the following equivalent conditions holds:
    \begin{enumerate}
        \item we have \(\lim_{x\to a} f(x)/g(x) = 0\);
        \item for all \(k>0\), there exists \(\delta\in(0,\infty)\) such that \(\abs{f(x)} \leq k\abs{g(x)}\) for all \(x\in N_{\delta}(a)\).\footnote{The \(k\) here actually belongs on the left side because it is \(g\) that is always greater no matter which multiple of \(f\) we take; it is placed on the right side just to make it appear similar to the definition of big-$O$.}
    \end{enumerate}
\end{definition}

The equivalence in this case follows by the definition of a limit: if \(\lim_{x\to a} f(x)/g(x) = 0\), then for all \(k>0\) there exists a \(\delta > 0\) such that \(\abs{f(x)/g(x)} < k\) for all \(x\in N_{\delta}(a)\), and vice versa.
Again, the definition given by Landau in \cite{Landau1909} assumes \(a=\infty\).

As with big-$O$, there is a simple way to understand little-$o$:
\begin{center}
    \(f(x) = o_a(g(x))\) means that \(kf(x) < g(x)\) in the limit for \textit{any} \(k\). \\
    (\(g\) dominates \(f\) \(\iff\) \(b\) is greater than \(a\))
\end{center}
Thus little-$o$ is stronger than big-$O$, in that \(f(x) = o_a(g(x))\) implies \(f(x) = O_a(g(x))\) but not vice versa.

\textbf{Examples:}

\begin{itemize}
    \item We have \(\sin x = o_{\infty}(x)\) as \(\sin x / x\) clearly tends to \(0\) as \(x\to\infty\).
        However, from elementary calculus we know that \(\lim_{x\to 0} \sin x/x = 1 \ne 0\), so \(\sin x \ne o_0(x)\).
        Similarly \(x \ne o_0(\sin x)\) either, as the limit of \(x/\sin x\) is also \(1\).
        We can see that \(\sin x = o_{\infty}(x)\) using the second condition too.
        Pick \(k > 0\), so we must find \(\delta\) such that \(\sin x \le kx\) for all \(x>\delta\).
        But this is easy: \(\sin x \le 1 = k \cdot 1/k \le kx\) for all \(x>1/k\), so \(\delta = 1/k\) works.

    \item If \(\lim_{x\to 0} f(x) = 0\), then \(f = o_0(1)\).
        In particular we have \(\sin x = o_0(1)\) and \(\tan x = o_0(1)\).

    \item Remember from last section that \(100x^2 = O_{\infty}(x^3)\).
        Since \(100x^2/x^3 = 100/x \to 0\) as \(x\to\infty\), we have \(100x^2 = o_{\infty}(x^3)\).
        However, note that while \(bx^3 = O_{\infty}(x^3)\) for any real \(b\), the same does not hold with big-$O$ replaced by little-$o$.

    \item We have \(f(x) = o_a(g(x))\) if and only if \(f(x+a) = o_0(g(x+a))\) for real functions \(f\) and \(g\) defined all over \(\RR\).
\end{itemize}

As with big-$O$, we will almost always omit the subscript and sometimes the variable when using little-$o$.

\begin{theorem}[Transitivity]
    If \(f = o(g)\) and \(g = o(h)\), then \(f = o(h)\).
\end{theorem}

\begin{proof}
    Given \(m\in(0,\infty)\), there exist \(\delta,\epsilon\in(0,\infty)\) such that \(\abs{f(x)}\le\sqrt m \abs{g(x)}\) for all \(x\in N_{\delta}(a)\) and \(\abs{g(x)}\le\sqrt m \abs{h(x)}\) for all \(x\in N_{\epsilon}(a)\).
    So we have
    \[\abs{f(x)} \le \sqrt m \abs{g(x)} \le m \abs{h(x)}\]
    for all \(x\in N_{\min(\delta,\epsilon)}\).
\end{proof}

\subsection{Bachmann-Landau equations}\label{ssec:equations}

We now define a new type of equation in which big-$O$'s and little-$o$'s are allowed to appear on either side any number of times, so equations like $x^2 + O(x^3)o(1) = o(x^4)$ will make sense.
Mathematicians usually do not distinguish between these so-called \textit{Bachmann-Landau} equations and normal equations, which creates some confusion.
A better way is to think of the class of normal equations as a subclass of the class of Bachmann-Landau equations; the latter make a claim about the existence of some functions satisfying certain properties.
The definition below describes a very general situtation without too much focus on formalisation, as that would distract from the topic at hand.

\begin{definition}
    Given \(a\in\RRE\), let \(X^i, Y^j\) represent any of \(O\) and \(o\) for \(i=1,2,\ldots,m\) and \(j=1,2,\ldots,n\).
    Let \(f_i, g_j\) be real functions defined and nonzero on a neighbourhood of \(a\), for \(i=1,2,\ldots,m\) and \(j=1,2,\ldots,n\).
    An equation of the form
    \[F(X^1(f_1(x)), X^2(f_2(x)), \ldots, X^m(f_m(x))) = G(Y^1(g_1(x)), Y^2(g_2(x)), \ldots, Y^n(g_n(x)))\]
    for some expressions \(F\) and \(G\) is called a \textit{Bachmann-Landau equation in} \(x\) and means the following: for any real functions \(f_1^*, \ldots, f_m^*\) defined on a neighbourhood of \(a\) such that \(f_i^*(x) = X^i(f_i(x))\), there exist real functions \(g_1^*, \ldots, g_n^*\) defined on an equal or larger neighbourhood of \(a\) such that \(g_j^*(x) = Y^j(g_j(x))\) and
    \[F(f_1^*(x), f_2^*(x), \ldots, f_m^*(x)) = G(g_1^*(x), g_2^*(x), \ldots, g_n^*(x))\]
    for all \(x\) for which the left side is defined.
\end{definition}

Note that Bachmann-Landau equations are \textit{not symmetric}, as the following examples show.
They are thus to be read left-to-right.

\textbf{Examples:}

\begin{itemize}
    \item Consider the Bachmann-Landau equation given above: \(x^2 + O(x^3)o(1) = o(x^4)\), as \(x\to\infty\).
        Let us check using the above definition that this equation holds.
        Take arbitrary real functions \(f\) and \(g\) defined everywhere such that \(\abs{f(x)} \leq k\abs{x}^3\) for all \(x\in(-\delta,\delta)\) for some finite \(\delta>0\) and \(\lim_{x\to 0} g(x) = 0\); we must verify that \(x^2 + f(x)g(x) = o(x^4)\), or in other words that \(\frac{x^2 + f(x)g(x)}{x^4}\to 0\) as \(x\to\infty\).
        Since
        \[\lim_{x\to 0} \frac{x^2 + f(x)g(x)}{x^4} = \lim_{x\to 0} \frac{x^2}{x^4} + \left(\lim_{x\to 0} g(x)\right)\left(\lim_{x\to 0} \frac{f(x)}{x^4}\right)\]
        if the very last limit converges, it suffices to show that \(f(x)/x^4 \to 0\) as \(x\to\infty\).
        But this is trivial: given \(\epsilon > 0\), we have \(\abs{f(x)/x^4} \le k/\abs{x} < \epsilon\) for all \(\abs{x} > k/\epsilon\).

    \item Let us verify that \(n^{O(1)} = e^{O(n)}\) as \(n\to\infty\); note that here the variable is \(n\) instead of \(x\), as is common in number theory, and note also that since left side might not make sense for \(n<0\) all domains will be \(\RR_{>0}\).
        Take any real function \(f\) defined on \(\RR_{>0}\) such that \(f = O(1)\); it suffices to show that \(f(n)\log n = O(n)\).
        By definition, there exist finite \(N,k>0\) such that \(n>N\) implies \(\abs{f(n)} \le k\), and we know that \(\log n < n\) for all \(n>1\).\footnote{That \(\log n < n\) for \(n\ge 1\) follows from the \textit{racetrack principle}; see \cite{Spivey2011} for details.}
        Thus for all \(n > \max(N,1)\) we have \(\abs{f(n)}\log n \le kn\), as desired.

    \item However, \(e^{O(n)} \ne n^{O(1)}\).
        This is to say, there exists some \(f(n) = O(n)\) for which there is no \(g(n) = O(1)\) satisfying \(e^{f(n)} = n^{g(n)}\).
        But this is not hard: just take \(f(n) = n\).
        Then the only possible function \(g\) would be \(g(n) = n\log_n(e) = n/\log n\), but it is easy to check that \(n/\log n \ne O(1)\).
        Of course, here \(a=\infty\).

    \item The Swedish mathematician Helge von Koch showed in \cite{Koch1901} that the Riemann hypothesis implies
        \[\pi(x) = \Li(x) + O(\sqrt x \log x);\]
        here \(\pi(x)\) is the number of primes from \(1\) to \(x\) and \(\Li(x) = \int_2^x dt/\log t\).
        This equation means that \(\abs{\pi(x) - \Li(x)} \le k\sqrt x \log x\) for some real \(k>0\) and all sufficiently large \(x\), or explicitly that
        \[\Li(x) - k\sqrt x \log x \le \pi(x) \le \Li(x) + \sqrt x\log x.\]

    \item If \(F=G\) and \(G=H\) are valid Bachmann-Landau equations, then so is \(F=H\) by the definition.
        This allows to write chains of equalities as we will do in \Cref{sec:applications}.
\end{itemize}

Fortunately, we do not have to go through such long calculations when we are dealing with Bachmann-Landau equations.
There are some recurring ways in which Bachmann-Landau equations are manipulated that are explained in the following two theorems.
Assume a fixed value of \(a\).

\begin{theorem}
    Let \(\lambda\in\RR\), and let \(f,g\) be real functions defined and nonzero on some neighbourhood of \(a\).
    Then we have \(O(f)O(g) = O(fg)\), \(fO(g) = O(fg)\), \(\lambda O(f) = O(f)\), and \(O(\lambda f) = O(f)\).
    If \(f\) and \(g\) are nonnegative, then \(O(f) + O(g) = O(\max(f, g))\) and in particular \(O(f) + O(f) = O(f)\).
\end{theorem}

Each of the equations in the statement of the above theorem is a Bachmann-Landau equation, as the proof makes clear.

\begin{proof}
    Suppose \(f_1 = O(f)\) and \(g_1 = O(g)\), so there exist \(\delta,\epsilon,k,l\in(0,\infty)\) such that \(\abs{f_1(x)} \leq k\abs{f(x)}\) for all \(x\in N_{\delta}(a)\) and \(\abs{g_1(x)} \leq l\abs{g(x)}\) for all \(x\in N_{\epsilon}(a)\).
    We know that \(N_{\delta}(a) \cap N_{\epsilon}(a) = N_{\gamma}(a)\) for some \(\gamma\in(0,\infty)\).\footnote{We have \(\gamma = \min(\delta,\epsilon)\) if \(a\) is finite, and \(\gamma = \pm\max(\delta,\epsilon)\) if \(a=\pm\infty\).}
    Let \(m=\max(k,l)\).
    Then \(\abs{f_1(x)/f(x)} \leq m\) and \(\abs{g_1(x)/g(x)} \leq m\) for all \(x\in N_{\gamma}(a)\), therefore
    \[\abs{\frac{f_1(x)g_1(x)}{f(x)g(x)}} \leq m^2\]
    for all \(x\in N_{\gamma}(a)\); this proves the first equation.
    The second and third equations follow from
    \[\abs{\frac{f(x)g_1(x)}{f(x)g(x)}} = \abs{\frac{g_1(x)}{g(x)}} \leq l\quad\forall x\in N_{\epsilon}(a) \qquad \text{and} \qquad \abs{\frac{\lambda f_1(x)}{f(x)}} = \abs{\lambda}\frac{f_1(x)}{f(x)} \leq k\abs{\lambda}\quad\forall x\in N_{\delta}(a).\]
    The fourth equation follows from the fact that
    \[\sup_{N_{\kappa}(a)} \abs{\frac{f_2(x)}{\lambda f(x)}} < \infty \implies \sup_{N_{\kappa}(a)} \abs{\frac{f_2(x)}{f(x)}} < \infty\]
    for any real function \(f_2\) defined on a neighbourhood of \(a\).
    Finally, if \(f\) and \(g\) are both nonnegative, then for all \(x\in N_{\gamma}(a)\) we have
    \begin{align*}
        \abs{f_1(x)+g_1(x)} &\leq \abs{f_1(x)} + \abs{g_1(x)} \\
        &\leq m(\abs{f(x)} + \abs{g(x)}) \\
        &= m(f(x)+g(x)) \\
        &\leq 2m\max(f(x),g(x)) = 2m\abs{\max(f(x), g(x))}.
    \end{align*}
\end{proof}

The exact same theorem holds with big-$O$ replaced by little-$o$.

\begin{theorem}
    Let \(\lambda\in\RR\), and let \(f,g\) be real functions defined and nonzero on some neighbourhood of \(a\).
    Then we have \(o(f)o(g) = o(fg)\), \(fo(g) = o(fg)\), \(\lambda o(f) = o(f)\), and \(o(\lambda f) = o(f)\).
    If \(f\) and \(g\) are nonnegative, then \(o(f) + o(g) = o(\max(f, g))\) and in particular \(o(f) + o(f) = o(f)\).
\end{theorem}

\begin{proof}
    Let \(f_1,g_1\) be real functions defined on a neighbourhood of \(a\) such that \(f_1 = o(f)\) and \(g_1 = o(g)\).
    Then \(\lim_{x\to a} f_1(x)/f(x) = \lim_{x\to a} g_1(x)/g(x) = 0\) implies
    \[\lim_{x\to a} \frac{f(x)g_1(x)}{f(x)g(x)} = \lim_{x\to a} \frac{g_1(x)}{g(x)} = 0\ \qquad \text{and} \qquad \lim_{x\to a} \frac{f_1(x)g_1(x)}{f(x)g(x)} = \left(\lim_{x\to a} \frac{f_1(x)}{f(x)}\right) \left(\lim_{x\to a} \frac{g_1(x)}{g(x)}\right) = 0;\]
    thus \(o(f)o(g)=o(fg)\) and \(fo(g)=o(fg)\).
    Also \(\lim_{x\to a} \lambda f_1(x)/f(x)\) implies \(\lambda o(f) = o(f)\), and if \(f_2 = o(\lambda f)\), then \(\lim_{x\to a} f_2(x)/(\lambda f(x)) = 0\) and therefore \(\lim_{x\to a} f_2(x)/f(x) = 0\), showing \(o(\lambda f) = o(f)\).
    Now suppose \(f\) and \(g\) are nonnegative, implying that are \(f_1\) and \(g_1\), and take any \(m\in(0,\infty)\).
    There exist \(\delta,\epsilon\in(0,\infty)\) such that we have \(f_1(x) \le mf(x)\) for all \(x\in N_{\delta}(a)\) and \(g_1(x) \le mg(x)\) for all \(x\in N_{\epsilon}(a)\).
    Then \(f_1(x) + g_1(x) \le m(f(x) + g(x))\) for all \(x\in N_{\zeta}(a)\), where \(\zeta = \min(\delta,\epsilon)\).
\end{proof}

\section{Application: Average Orders of Divisor Functions}\label{sec:applications}

To demonstrate the use of Bachmann-Landau notation in everyday mathematics, we explore some very basic estimates in analytic number theory that require big-$O$.
These are taken from Chapter 3 of \cite{Apostol1976} and involve the partial summations of standard arithmetic functions.

We first define all the relevant notions that will be required.

\begin{definition}
    \item
    \begin{enumerate}
        \item Given a natural number \(n\), let \(\tau(n)\) denote the number of positive divisors of \(n\), and given \(s\in\RR\) let \(\sigma_s(n)\) denote the sum of the \(s\)-th powers of these divisors.

        \item For \(s\in\RR_{>0}\), define
            \[
                \zeta(s) =
                \begin{cases}
                    \sum_{n=1}^{\infty} n^{-s} & \text{if } s > 1 \\
                    \lim_{x\to\infty} \left(\sum_{n\le x} \frac{1}{n^s} - \frac{x^{1-s}}{1-s}\right) & \text{if } 0 < s < 1.
                \end{cases}
            \]
            This is called the \textit{Riemman Zeta-Function}.

        \item Define \(C = \lim_{n\to\infty} (1 + 1/2 + 1/3 + \ldots + 1/n - \log n)\).
            This is called \textit{Euler's constant}.
    \end{enumerate}
\end{definition}

There is a special way to write such functions that run over the divisors of a variable number.
We write
\[\tau(n) = \sum_{d\mid n} 1 \qquad \text{and} \qquad \sigma_s(n) = \sum_{d\mid n} d^s.\]
In particular, this clarifies the fact that \(\sigma_0\) and \(\tau\) are the same function, called the \textit{divisor counting function}.
Also \(\sigma_1\) is often written just as \(\sigma\) and called the \textit{divisor sum function}.
The function \(\zeta\) is at the heart of mathematics and especially number theory.
We touch not even the tip of the iceberg encompassing the work that has been done regarding this function.
Note that the infinite sum \(\sum n^{-s}\) converges if and only if \(s>1\), and thus the case \(0<s<1\) has to be considered separately; we will not go into detail about how this second expression is derived.
As for the constant \(C\), it features in the theorems below and is not to be confused with the constant \(e = \lim_{n\to\infty} (1+1/n)^n\).
Although it is not strictly necessary, we will use the fact that \(\zeta(2) = \pi^2/6\).
This statement is known for having numerous beautiful proofs that can easily be found on the Internet.

One of the goals of number theoretic research is to understand deeply the behaviour of arithmetic functions like \(\tau\) and \(\sigma_s\), and analytic number theory focuses on their behaviour for large \(n\).
However, functions in number theory are notorious for fluctuating; \(\tau\), for example, takes the value \(2\) for every prime \(n\), of which there are infinitely many.
So instead of focusing on \(\tau(n)\) specifically, we can instead question the \textit{average} value of \(\tau(n)\) over a large sample of numbers, leading to the following.

\begin{definition}
    Given a function \(f:\mathbb N\to \mathbb R\), the \textit{average order} of \(f\) is the function \(\tilde{f}(x) = \frac 1 x\sum_{n\le x} f(n)\).
\end{definition}

In turns out that we can compute big-$O$ estimates for \(\tilde{\tau}\) and \(\tilde{\sigma_s}\), but before that we need some basic results.

\begin{theorem}[Euler's Summation Formula]\label{theorem:esf}
    Given a real function \(f:\RR\to\RR\) that is continuously differentiable on the interval \([x,y]\), we have
    \[\sum_{x<n\le y} f(n) = \int_x^y f(t)\,dt + \int_x^y (t-\fl{t}) f'(t)\,dt + f(x)(x - \fl{x}) - f(y)(y - \fl{y}).\]
\end{theorem}

\begin{proof}
    Observe that
    \begin{align*}
        \int_{\fl x}^{\fl y} \fl t f'(t)\,dt &= \sum_{n = \fl x + 1}^{\fl y} \int_{n-1}^n \fl t f'(t)\,dt \\
        &= \sum_{n = \fl x + 1}^{\fl y} \int_{n-1}^n (n-1)f'(t)\,dt \\
        &= \sum_{n = \fl x + 1}^{\fl y} (n-1)(f(n) - f(n-1)) \\
        &= \fl y f(\fl y) - \fl x f(\fl x) - \sum_{x<n\le y} f(n),
    \end{align*}
    so
    \begin{align*}
        \sum_{x<n\le y} f(n) &= \fl y f(\fl y) - \fl x f(\fl x) - \int_{\fl x}^{\fl y} \fl t f'(t)\,dt \\
        &= \fl y f(\fl y) - \fl x f(\fl x) - \left(\int_x^y \fl t f'(t)\,dt + \int_{\fl x}^{x} \fl t f'(t)\,dt - \int_{\fl y}^{y} \fl t f'(t)\,dt\right) \\
        &= \fl y f(\fl y) - \fl x f(\fl x) - \left(\int_x^y \fl t f'(t)\,dt + \fl x (f(x) - f(\fl x)) - \fl y (f(y) - f(\fl y))\right) \\
        &= \fl y f(y) - \fl x f(x) - \left(\int_x^y \fl t f'(t)\,dt\right). \\
    \end{align*}
    Now integration by parts gives
    \[\int_x^y tf'(t)\,dt = \int_x^y f(t)\,dt + yf(y) - xf(x),\]
    and adding this to the last expression finishes the proof.
\end{proof}

\begin{theorem}[Exponential Sums]\label{theorem:expsums}
    For \(x\ge 1\) we have
    \begin{enumerate}
        \item \(\sum_{n\le x} 1/n = \log x + C + O(1/x)\);
        \item \(\sum_{n\le x} 1/n^s = x^{1-s}/(1-s) + \zeta(s) + O(x^{-s})\) if \(s\in\RR_{>0}-\{1\}\);
        \item \(\sum_{n>x} 1/n^s = O(x^{1-s})\) if \(s>1\);
        \item \(\sum_{n\le x} n^u = x^{u+1}/(u+1) + O(x^u)\) if \(u\ge 0\).
    \end{enumerate}
\end{theorem}

Let us first of all understand what these equations signify.
The first of the above equations says that if we define the function \(F:\RR_{>1}\to\RR\) by \(F(x) = \sum_{n\le x} 1/n\), then the function \(F(x) - \log x - C\) grows slower than \(1/x\) as \(x\to\infty\).
More precisely, there exists a constant \(k\) such that for all sufficently large \(x\) we have
\[C + \log x - \frac k x \le F(x) \le C + \log x + \frac k x.\]
The other three equations can similarly be translated into such inequalities.

\begin{proof}
\item
    \begin{enumerate}
        \item Taking \(f(t) = 1/t\) in \Cref{theorem:esf} gives
            \begin{align*}
                \sum_{n<x} \frac 1 n &= \int_1^x \frac{dt}{t} - \int_1^x \frac{t-\fl t}{t^2}\,dt + 1 - \frac{x-\fl x}{x} \\
                &= \log x + \left(1 - \int_1^x \frac{t-\fl t}{t^2}\,dt\right) + O\left(\frac 1 x\right)
            \end{align*}
            Here we used the fact that \(-(x-\fl x)/x = O(1/x)\), which is true because \(x-\fl x \le 1\) for all \(x\).\footnote{So the ratio is clearly bounded above (by $1$).}

            Now observe that
            \[\int_1^x \frac{t-\fl t}{t^2}\,dt = \int_1^{\infty} \frac{t-\fl t}{t^2}\,dt - \int_x^{\infty} \frac{t-\fl t}{t^2}\,dt,\]
            where the first improper integral converges by the comparison test with \(\int_1^{\infty} t^{-2}\,dt\).
            Denote the second integral as a function \(G(x)\) for \(x\ge 1\).
            We have \(0\le G(x) \le \int_x^{\infty} t^{-2}\,dt = 1/x\), so actually
            \[\int_1^x \frac{t-\fl t}{t^2}\,dt = \int_1^{\infty} \frac{t-\fl t}{t^2}\,dt + O\left(\frac 1 x \right)\]
            and putting this in the previous equation we get
            \[\sum_{n<x} \frac 1 n = \log x + \left(1 - \int_1^{\infty} \frac{t-\fl t}{t^2}\,dt\right) + O\left(\frac 1 x\right).\]
            Note that the two \(O(1/x)\) have been combined into one.
            Moving \(\log x\) to the left side and taking the limit as \(x\to\infty\), the \(O(1/x)\) term disappears and we are left with
            \[\left(1 - \int_1^{\infty} \frac{t-\fl t}{t^2}\,dt\right) = \lim_{x\to\infty} \left(\sum_{n<x} \frac 1 n - \log x\right) = C,\]
            finishing the proof.

        \item Let \(f(t) = 1/t^s\) in \Cref{theorem:esf} and observe as in part (a) that
            \[\sum_{n\le x} \frac{1}{n^s} &= \frac{x^{1-s}}{1-s} + C(s) + O(x^{-s}),\]
            where
            \[C(s) = 1 - \frac{1}{1-s} - s\int_1^{\infty} \frac{t-\fl t}{t^{s+1}}\,dt.\]
            If \(s>1\), then in the limit the left side converges to \(\zeta(s)\) while the nonconstant terms on the right vanish, leaving \(C(s) = \zeta(s)\).
            If \(s<1\), then moving the \(x^{1-s}\) term to the left and taking the limit again gives \(C(s) = \zeta(s)\), finishing the proof.

        \item We have
            \[\sum_{n>x} \frac{1}{n^s} = \zeta(s) - \sum_{n\le x} \frac{1}{n^s} = \frac{x^{1-s}}{s-1} + O(x^{-s}) = O(x^{1-s})\]
            from part (b).

        \item Simply put \(f(t) = t^u\) in \Cref{theorem:esf} and reduce as in parts (a) and (b).
    \end{enumerate}
\end{proof}

Using these results it is now possible to prove the two theorems below that determine the average order of \(\tau\) and \(\sigma_s\).
Unfortunately, the proof will not be given here due to the lack of space, but it can be found, as mentioned above, in Chapter 3 of \cite{Apostol1976}.

\begin{theorem}[Dirichlet's Formula]\label{theorem:tau}
    For \(x\ge 1\) we have
    \[\sum_{n\le x} \tau(n) = x\log x + (2C-1)x + O(\sqrt x)\]
\end{theorem}

Although the error term \(O(\sqrt x)\) has been improved to smaller powers of \(x\), the infimum of all \(u\) for which the error term is \(O(x^u)\) is an open problem called \textit{Dirichlet's divisor problem}.

\begin{theorem}[Divisor Sum Estimates]\label{theorem:sigma}
    For \(x\ge 1\) we have
    \[
        \sum_{n\le x} \sigma_s(n) =
        \begin{cases}
            \pi^2x^2/12 + O(x\log x) & \text{if } s=1 \\
            \pi^2x/6 + O(\log x) & \text{if } s=-1 \\
            \zeta(s+1)x^{s+1}/(s+1) + O(x^{\max(1,s)}) & \text{if } s>0 \text{ and } s\ne 1 \\
            \zeta(1-s)x + O(x^{\max(0,1+s)}) & \text{if } s<0 \text{ and } s\ne -1.
        \end{cases}
    \]
\end{theorem}

As a direct corollary, we get the average orders; simply divide by a factor of \(x\) in each of the above cited formulas.
Note that we ignore the error term when mentioning the average order, so the average order of \(\tau\) is \(\log x\) and that of \(\sigma_1\) is \(\pi^2x/12\).

\begin{theorem}
    For \(x\ge 1\) we have \(\tilde{\tau}(x) = \log x + (2C-1) + O(x^{-1/2})\) and
    \[
        \tilde{\sigma_s}(x) =
        \begin{cases}
            \pi^2x/12 + O(\log x) & \text{if } s=1 \\
            \pi^2/6 + O(\log x/x) & \text{if } s=-1 \\
            \zeta(s+1)x^s/(s+1) + O(x^{\max(0,s-1)}) & \text{if } s>0 \text{ and } s\ne 1 \\
            \zeta(1-s) + O(x^{\max(-1,s)}) & \text{if } s<0 \text{ and } s\ne -1.
        \end{cases}
    \]
\end{theorem}

The above result can be intepreted as saying that as \(x\to\infty\), the function \(\tau\) is ``on average'' \(\log x + (2C-1)\), since the error term goes to \(0\) in the limit;
similarly, \(\sigma_{-1}\) is ``on average'' \(\pi^2/6\) for large \(x\).

\section{Conclusion}\label{ssec:conclusion}

In this investigation, I defined big-$O$ and little-$o$ from scratch, discussed the theoretical aspects of their usage, and concluded with a sustained example of how big-$O$ is used in real mathematics.
This example, however, only discussed infinite asymptotic estimates using big-$O$ and no other uses: big-$O$ and little-$o$ are both often used to deal with error terms in infinitesimal limits, for instance, but the page limit did not allow me to give further examples of such usage.
My attempt at formalising equations containing Bachmann-Landau notation also leaves much to be desired, due to its apparent vagueness.
The term ``expression'' is not explained; just what constitutes an expression?
I intentionally avoided trying to be too precise because it would have lead me into a foundational rabbithole, as is only too common in mathematics these days.
These issues, among others, could be improved upon in a more lengthy work on the topic.
At the very least, I will not forget what \(O\) and \(o\) mean now.

\nocite{*}
\printbibliography

\end{document}
